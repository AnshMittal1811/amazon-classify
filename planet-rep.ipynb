{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "\n",
    "# Housekeeping imports\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score\n",
    "from keras import backend\n",
    "\n",
    "# Images\n",
    "from matplotlib import pyplot \n",
    "from matplotlib.image import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size\n",
    "pyplot.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_first_nine(folder = 'train-jpg/'):\n",
    "    '''\n",
    "    Input: File path to training data.\n",
    "    Output: Plot of first 9 images in training data.\n",
    "    \n",
    "    We plot 9 images because otherwise we will have to\n",
    "    write formatting code for the sub-plot which is not\n",
    "    worth the time.\n",
    "    '''\n",
    "    for i in range(9):\n",
    "        # pyplot.subplot takes 3 digit code \n",
    "        # The first number is the number of rows\n",
    "        # The second number is the number of columns\n",
    "        # The third number is the position in the subplot \n",
    "        pyplot.subplot(340 + 1 + i)\n",
    "        # Define filename\n",
    "        filename = folder + 'train_' + str(i) + '.jpg'\n",
    "        # Load image pixels\n",
    "        image = imread(filename)\n",
    "        # Plot raw pixel data\n",
    "        pyplot.imshow(image)\n",
    "    # Show the figure\n",
    "    pyplot.show()\n",
    "    # End the function\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapping_data(file_name = 'train_v2.csv'):\n",
    "    '''\n",
    "    Input: mapping file name\n",
    "    Output: mapping file data-frame\n",
    "    '''\n",
    "    mapping = pd.read_csv(file_name)\n",
    "    print(mapping.shape)\n",
    "    print(mapping.head())\n",
    "    return(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_mapping(mapping):\n",
    "    '''\n",
    "    Input: mapping data-frame\n",
    "    Output: Dictionary mapping labels to integers.\n",
    "    '''\n",
    "    # Initialize labels\n",
    "    # Labels is a set so calling update will not affect uniqueness\n",
    "    labels = set()\n",
    "    \n",
    "    # Loop through the data-frame\n",
    "    # Split the tag values on spaces\n",
    "    # Then update the set with the tags\n",
    "    for i in range(len(mapping)):\n",
    "        tags = mapping['tags'][i].split(' ')\n",
    "        labels.update(tags)\n",
    "    \n",
    "    # Turn into a list and sort\n",
    "    labels = list(labels)\n",
    "    labels.sort()\n",
    "    \n",
    "    # First relate labels to integers\n",
    "    labels_map = {labels[i]:i for i in range(len(labels))}\n",
    "    inv_labels_map = {i:labels[i] for i in range(len(labels))}\n",
    "    \n",
    "    # Return statement\n",
    "    return(labels_map, inv_labels_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_mapping(mapping_df):\n",
    "    '''\n",
    "    Input: mapping data-frame\n",
    "    Output: mapping dictionary of filename to tags\n",
    "    '''\n",
    "    # Initialize the dictionary\n",
    "    mapping = dict()\n",
    "    # Iterate through the data-frame range\n",
    "    for i in range(len(mapping_df)):\n",
    "        # Store names and tags\n",
    "        name, tags = mapping_df['image_name'][i], mapping_df['tags'][i]\n",
    "        # Put them in the dictionary as name:tag key value pairs\n",
    "        mapping[name] = tags.split(' ')\n",
    "    \n",
    "    # Return mapping\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(tags, mapping):\n",
    "    '''\n",
    "    There are 17 elements in tag. \n",
    "    We want a 17 element vector of 0s and 1s.\n",
    "    Each element should be 1 if the corresponding\n",
    "    category is in the image file and 0 otherwise.\n",
    "    '''\n",
    "    # Create empty vector\n",
    "    encoding = np.zeros(len(mapping), dtype='uint8')\n",
    "    # Mark 1 for each tag in the vector\n",
    "    for tag in tags:\n",
    "        encoding[mapping[tag]] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_dataset(path, file_mapping, tag_mapping, target_size = (128, 128)):\n",
    "    '''\n",
    "    Inputs: 1) Folder: Path to training folder\n",
    "            2) Images file path: Path to training data images within training folder\n",
    "            3) File mapping: Mapping of training images to their labels\n",
    "            4) Tag mapping:  One to one mapping of labels to integers\n",
    "            5) Target size:  Size that input images should be cropped to\n",
    "    Output: 1) Training data and labels as numpy arrays of unsigned integers\n",
    "    \n",
    "    Why is path separately specified? \n",
    "    This is probably to ensure that if images are stored remotely like on Amazon S3 \n",
    "    then this parameter can be easily changed to get data from there. \n",
    "    '''\n",
    "    # Photos and targets stored here\n",
    "    photos, targets = list(), list()\n",
    "    # Enumerate files in the directory\n",
    "    for filename in os.listdir(folder):\n",
    "        # Load image\n",
    "        photo = keras.preprocessing.image.load_img(path + filename, target_size=target_size)\n",
    "        # Convert to numpy array\n",
    "        photo = keras.preprocessing.image.img_to_array(photo, dtype='uint8')\n",
    "        # Get tags\n",
    "        tags = file_mapping[filename[:-4]]\n",
    "        # One hot encode tags\n",
    "        target = one_hot_encode(tags, tag_mapping)\n",
    "        # Store photos \n",
    "        photos.append(photo)\n",
    "        # Store targets\n",
    "        targets.append(target) \n",
    "    # We know color channel values go from 0 to 255 and will not be negative\n",
    "    # Convert to arrays while making data type unsigned\n",
    "    # Unsigned integer saves space \n",
    "    X = np.asarray(photos, dtype='uint8')\n",
    "    y = np.asarray(targets, dtype='uint8')\n",
    "    \n",
    "    return(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data():    \n",
    "    '''\n",
    "    Input: None \n",
    "    Output: None \n",
    "    Description: \n",
    "    This is a function that is run for its side effect.\n",
    "    It runs the code to take in raw images and return single compressed file.\n",
    "    '''\n",
    "    # Plot the first nine images\n",
    "    plot_first_nine()\n",
    "    # Create mapping data-frame\n",
    "    mapping_df = load_mapping_data()\n",
    "    # Create tag mapping\n",
    "    labels_map, inv_labels_map = create_tag_mapping(mapping_df)\n",
    "    # Create file mapping\n",
    "    mapping = create_file_mapping(mapping_df)\n",
    "    # Load the jpeg images\n",
    "    folder = 'train-jpg/'\n",
    "    # Set the target_size\n",
    "    target_size = (32, 32)\n",
    "    # Load data-set\n",
    "    X, y = compress_dataset(folder, mapping, labels_map, target_size)\n",
    "    # Print X.shape, y.shape\n",
    "    print(X.shape, y.shape)\n",
    "    # Save both arrays to one file in compressed format\n",
    "    np.savez_compressed('planet_data.npz', X, y)\n",
    "    # End of function\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split, metric definition and benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    '''\n",
    "    Input: N/A\n",
    "    Output: Planet data\n",
    "    '''\n",
    "    # Load dataset\n",
    "    data = np.load('planet_data.npz')\n",
    "    X, y = data['arr_0'], data['arr_1']\n",
    "\n",
    "    # Separate into train and test datasets\n",
    "    trainX, testX, trainY, testY = sklearn. \\\n",
    "                                   model_selection. \\\n",
    "                                   train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    # Print out shapes as a sanity check\n",
    "    print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "    # Return splits\n",
    "    return(trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fbeta score for multi-class/label classification\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "    '''\n",
    "    This function is manually written here.\n",
    "    This is because this competition uses F-beta score as a metric.\n",
    "    This metric is no longer supported by Keras as v.2.0.0. There is\n",
    "    a Kaggle kernel which proposes the function given below as a fix\n",
    "    for this. Until then we use the function code given in the post\n",
    "    to measure our model's performance.\n",
    "    \n",
    "    Open questions: \n",
    "    What is the keras.backend module? \n",
    "    What does karas.backend.clip do? \n",
    "    What does keras.backend.round do? \n",
    "    What does keras.backend.epsilon do? \n",
    "    '''\n",
    "    # Clip predictions\n",
    "    y_pred = keras.backend.clip(y_pred, 0, 1)\n",
    "    # Calculate true positives\n",
    "    tp = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    # Calculate false positives\n",
    "    fp = keras.backend.sum(keras.backend.round(keras.backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "    # Calculate false negatives\n",
    "    fn = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "    # Calculate precision\n",
    "    p = tp / (tp + fp + keras.backend.epsilon())\n",
    "    # Calculate recall\n",
    "    r = tp / (tp + fn + keras.backend.epsilon())\n",
    "    # Calculate fbeta, averaged across each class\n",
    "    bb = beta ** 2\n",
    "    # F-beta score final calculation\n",
    "    fbeta_score = keras.backend.mean((1 + bb) * (p * r) / (bb * p + r + keras.backend.epsilon()))\n",
    "    # Return statement\n",
    "    return(fbeta_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(trainX, trainY, testX, testY):\n",
    "    \n",
    "    # Make all one predictions\n",
    "    train_yhat = np.asarray([np.ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\n",
    "    test_yhat = np.asarray([np.ones(testY.shape[1]) for _ in range(testY.shape[0])])\n",
    "    \n",
    "    # Evaluate predictions with sklearn\n",
    "    train_score = fbeta_score(trainY, train_yhat, 2, average='samples')\n",
    "    test_score = fbeta_score(testY, test_yhat, 2, average='samples')\n",
    "    print('All Ones (sklearn): train=%.3f, test=%.3f' % (train_score, test_score))\n",
    "\n",
    "    # Evaluate predictions with keras\n",
    "    train_score = fbeta(keras.backend.variable(trainY), keras.backend.variable(train_yhat))\n",
    "    test_score = fbeta(keras.backend.variable(testY), keras.backend.variable(test_yhat))\n",
    "    print('All Ones (keras): train=%.3f, test=%.3f' % (train_score, test_score))\n",
    "    \n",
    "    # Return the train and test sets for future use\n",
    "    return(train_score, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28335, 32, 32, 3) (28335, 17) (12144, 32, 32, 3) (12144, 17)\n",
      "All Ones (sklearn): train=0.484, test=0.484\n",
      "All Ones (keras): train=0.484, test=0.484\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "trainX, trainY, testX, testY = load_dataset()\n",
    "# Run benchmark\n",
    "results = benchmark(trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline convolutional neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
