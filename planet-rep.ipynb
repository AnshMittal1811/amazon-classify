{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This notebook will replicate this program from Jason Brownlee's Machine Learning Mastery post given [here](https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-satellite-photos-of-the-amazon-rainforest/). The aim is to get an understanding of how to work with simple .jpg files in making machine learning algorithms. The task in this post is to take an image as input and predict whether that image contains one of seventeen land types. The inputs are (128 X 128) images of Amazon rainforest area. The post makes a convolutional neural net to make predictions.\n",
    "\n",
    "\n",
    "The first set of code prepares the data. This code is meant to be run only once to load the images, convert them into numerical arrays, compress them, and store them in a format called .npz. Because it is meant to be run only once, currently there are no function calls for this code. \n",
    "\n",
    "The second part of the code is the model set-up and training. Everything is the same as in the blog post except for we have removed one hidden layer from the neural network just to speed up training to get the process correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Housekeeping\n",
    "\n",
    "The block below imports the packages needed for preparing the data. The imports all get used in the code. **Pandas** is needed to read .csv files. **Numpy** provides fast numerical array operations. **Keras** is the deep learning framework we are using. **OS** allows us to perform actions like listing all the files in a particular folder. **Matplotlib** lets us make model evaluation plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot \n",
    "from matplotlib.image import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below sets the size of the plot. The default plot size in notebooks is very big. This takes care of that by making the plots smaller and easier to see. This makes sense here since we are going to be making only simple plots like looking at satellite imagery or training/test error plots which don't need to be very huge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size\n",
    "pyplot.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the images\n",
    "\n",
    "We first start off by writing a function which plots the first 9 images that are located in the image folder. We want to plot the first 9 purely for reasons of convenience. We will otherwise have to write formatting code for the subplots which is not worth the time. This function plots the raw pixel data of the .jpg images in the notebook. The **imread** function from matplotlib loads the image. The **imshow** function in pyplot creates the plot and adds it to the sub-plot. The entire sub-plot is shown once the loop exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_first_nine(folder = 'train-jpg/'):\n",
    "    '''\n",
    "    Input: File path to training data.\n",
    "    Output: Plot of first 9 images in training data.\n",
    "    \n",
    "    We plot 9 images because otherwise we will have to\n",
    "    write formatting code for the sub-plot which is not\n",
    "    worth the time.\n",
    "    '''\n",
    "    for i in range(9):\n",
    "        # pyplot.subplot takes 3 digit code \n",
    "        # The first number is the number of rows\n",
    "        # The second number is the number of columns\n",
    "        # The third number is the position in the subplot \n",
    "        pyplot.subplot(340 + 1 + i)\n",
    "        # Define filename\n",
    "        filename = folder + 'train_' + str(i) + '.jpg'\n",
    "        # Load image pixels\n",
    "        image = imread(filename)\n",
    "        # Plot raw pixel data\n",
    "        pyplot.imshow(image)\n",
    "    # Show the figure\n",
    "    pyplot.show()\n",
    "    # End the function\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a mapping data-frame\n",
    "\n",
    "**train_v2.csv** is a .csv file that has two columns. The first column is the name of the image. The second column is a string that contains all the labels that apply to that image. An example first column observation is **'train_0.jpg'.** An example second column observation is **'haze forest cloud'.** The function below takes the file name as input and returns a pandas data-frame version of this file for the code to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mapping_data(file_name = 'train_v2.csv'):\n",
    "    '''\n",
    "    Input: mapping file name\n",
    "    Output: mapping file data-frame\n",
    "    '''\n",
    "    mapping = pd.read_csv(file_name)\n",
    "    print(mapping.shape)\n",
    "    print(mapping.head())\n",
    "    return(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating tag mapping\n",
    "\n",
    "The labels are given in the form of strings. We need to convert them into numbers because our eventual machine learning model will operate on numerical values. The function below takes the mapping data-frame and first gets the unique labels that occur in the data-frame. It then creates two dictionaries. The first dictionary relates string labels to integer values and the second dictionary goes the other way around and relates integer values to labels. These dictionaries are then returned for the code to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_mapping(mapping):\n",
    "    '''\n",
    "    Input: mapping data-frame\n",
    "    Output: Dictionary mapping labels to integers.\n",
    "    '''\n",
    "    # Initialize labels\n",
    "    # Labels is a set so calling update will not affect uniqueness\n",
    "    labels = set()\n",
    "    \n",
    "    # Loop through the data-frame\n",
    "    # Split the tag values on spaces\n",
    "    # Then update the set with the tags\n",
    "    for i in range(len(mapping)):\n",
    "        tags = mapping['tags'][i].split(' ')\n",
    "        labels.update(tags)\n",
    "    \n",
    "    # Turn into a list and sort\n",
    "    labels = list(labels)\n",
    "    labels.sort()\n",
    "    \n",
    "    # First relate labels to integers\n",
    "    labels_map = {labels[i]:i for i in range(len(labels))}\n",
    "    inv_labels_map = {i:labels[i] for i in range(len(labels))}\n",
    "    \n",
    "    # Return statement\n",
    "    return(labels_map, inv_labels_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a mapping dictionary\n",
    "\n",
    "We have the labels associated with each file in the data-frame but we need these labels to be in the form of a dictionary to use in the deep learning model later. This function takes in the data-frame and then goes through it row by row. For each row it extracts the file name and the tags and puts them in separate variables. It then adds a new entry to a mapping dictionary where the key is the file name and the tag is the list of tags. Note that the split command used below takes the dataframe string entry and splits it on spaces and then finally creates a list from it. This is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_mapping(mapping_df):\n",
    "    '''\n",
    "    Input: mapping data-frame\n",
    "    Output: mapping dictionary of filename to tags\n",
    "    '''\n",
    "    # Initialize the dictionary\n",
    "    mapping = dict()\n",
    "    # Iterate through the data-frame range\n",
    "    for i in range(len(mapping_df)):\n",
    "        # Store names and tags\n",
    "        name, tags = mapping_df['image_name'][i], mapping_df['tags'][i]\n",
    "        # Put them in the dictionary as name:tag key value pairs\n",
    "        mapping[name] = tags.split(' ')\n",
    "    \n",
    "    # Return mapping\n",
    "    return mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do one-hot encoding of the labels. This is because there are 17 possible tags. We want to create a vector of 0s and 1s such that an element in the vector takes the value 1 only if the label of that image corresponds to the position of the element. So say that the label of a particular image is 'forest' which has integer value 7. Then in the 17 element vector we want only the 7th element to be 1 and the rest of the element values to be zero. This function takes in tags and the tag to integer mapping and returns the needed encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(tags, mapping):\n",
    "    '''\n",
    "    There are 17 elements in tag. \n",
    "    We want a 17 element vector of 0s and 1s.\n",
    "    Each element should be 1 if the corresponding\n",
    "    category is in the image file and 0 otherwise.\n",
    "    '''\n",
    "    # Create empty vector\n",
    "    encoding = np.zeros(len(mapping), dtype='uint8')\n",
    "    # Mark 1 for each tag in the vector\n",
    "    for tag in tags:\n",
    "        encoding[mapping[tag]] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compress the dataset\n",
    "\n",
    "We need to compress the dataset because it may cause local laptop to crash. This function iterates through the folder. For each image it uses the **keras** function **load_img** to load this. Then it converts this to a numerical **numpy array** using the **img_to_array** function which is also from **keras.** Then we retreieve the tags from the image. Finally we one-hot encode these tags. Then we append to the pre-created list of **photos** and **targets** respectively. Finally we convert these lists into **numpy arrays** of **unsigned integers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_dataset(path, file_mapping, tag_mapping, target_size = (128, 128)):\n",
    "    '''\n",
    "    Inputs: 1) Folder: Path to training folder\n",
    "            2) Images file path: Path to training data images within training folder\n",
    "            3) File mapping: Mapping of training images to their labels\n",
    "            4) Tag mapping:  One to one mapping of labels to integers\n",
    "            5) Target size:  Size that input images should be cropped to\n",
    "    Output: 1) Training data and labels as numpy arrays of unsigned integers\n",
    "    \n",
    "    Why is path separately specified? \n",
    "    This is probably to ensure that if images are stored remotely like on Amazon S3 \n",
    "    then this parameter can be easily changed to get data from there. \n",
    "    '''\n",
    "    # Photos and targets stored here\n",
    "    photos, targets = list(), list()\n",
    "    # Enumerate files in the directory\n",
    "    for filename in os.listdir(folder):\n",
    "        # Load image\n",
    "        photo = keras.preprocessing.image.load_img(path + filename, target_size=target_size)\n",
    "        # Convert to numpy array\n",
    "        photo = keras.preprocessing.image.img_to_array(photo, dtype='uint8')\n",
    "        # Get tags\n",
    "        tags = file_mapping[filename[:-4]]\n",
    "        # One hot encode tags\n",
    "        target = one_hot_encode(tags, tag_mapping)\n",
    "        # Store photos \n",
    "        photos.append(photo)\n",
    "        # Store targets\n",
    "        targets.append(target) \n",
    "    # We know color channel values go from 0 to 255 and will not be negative\n",
    "    # Convert to arrays while making data type unsigned\n",
    "    # Unsigned integer saves space \n",
    "    X = np.asarray(photos, dtype='uint8')\n",
    "    y = np.asarray(targets, dtype='uint8')\n",
    "    \n",
    "    return(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "\n",
    "The function below just calls the functions from above to execute the pipeline. Note that the target size default value is 32 X 32 to save space assuming that you will run this notebook on your local machine. There is no function call as of now because this is a one time-task. A file called 'planet-data.npz' which is the compressed version of the data-set is stored in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(folder = 'train-jpg/', target_size = (32, 32)):    \n",
    "    '''\n",
    "    Input: None \n",
    "    Output: None \n",
    "    Description: \n",
    "    This is a function that is run for its side effect.\n",
    "    It runs the code to take in raw images and return single compressed file.\n",
    "    '''\n",
    "    # Plot the first nine images\n",
    "    plot_first_nine()\n",
    "    # Create mapping data-frame\n",
    "    mapping_df = load_mapping_data()\n",
    "    # Create tag mapping\n",
    "    labels_map, inv_labels_map = create_tag_mapping(mapping_df)\n",
    "    # Create file mapping\n",
    "    mapping = create_file_mapping(mapping_df)\n",
    "    # Load data-set\n",
    "    X, y = compress_dataset(folder, mapping, labels_map, target_size)\n",
    "    # Print X.shape, y.shape\n",
    "    print(X.shape, y.shape)\n",
    "    # Save both arrays to one file in compressed format\n",
    "    np.savez_compressed('planet_data.npz', X, y)\n",
    "    # End of function\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Housekeeping imports\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "First we have to load the compressed data-set we created earlier and split it into training and testing data. The function below does this using the sci-kit model selection module. It prints out the shape of the data-sets as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    '''\n",
    "    Input: N/A\n",
    "    Output: Planet data split into train and test\n",
    "    '''\n",
    "    # Load dataset\n",
    "    data = np.load('planet_data.npz')\n",
    "    X, y = data['arr_0'], data['arr_1']\n",
    "\n",
    "    # Separate into train and test datasets\n",
    "    trainX, testX, trainY, testY = sklearn. \\\n",
    "                                   model_selection. \\\n",
    "                                   train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "    # Print out shapes as a sanity check\n",
    "    print(trainX.shape, trainY.shape, testX.shape, testY.shape)\n",
    "    # Return splits\n",
    "    return(trainX, trainY, testX, testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom metric\n",
    "\n",
    "Keras v2.0.0 currently does not support the F-beta loss function for multi-label classification. The F-beta loss function is a weighted average of precision and recall. Over here we use it only as a metric to measure the performance of the model but not in the actual training. The winning score in the competition is approximately 93%. The function given below is just coding this function manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fbeta score for multi-class/label classification\n",
    "def fbeta(y_true, y_pred, beta=2):\n",
    "    '''\n",
    "    This function is manually written here.\n",
    "    This is because this competition uses F-beta score as a metric.\n",
    "    This metric is no longer supported by Keras as v.2.0.0. There is\n",
    "    a Kaggle kernel which proposes the function given below as a fix\n",
    "    for this. Until then we use the function code given in the post\n",
    "    to measure our model's performance.\n",
    "    \n",
    "    Open questions: \n",
    "    What is the keras.backend module? \n",
    "    What does karas.backend.clip do? \n",
    "    What does keras.backend.round do? \n",
    "    What does keras.backend.epsilon do? \n",
    "    '''\n",
    "    # Clip predictions\n",
    "    y_pred = keras.backend.clip(y_pred, 0, 1)\n",
    "    # Calculate true positives\n",
    "    tp = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
    "    # Calculate false positives\n",
    "    fp = keras.backend.sum(keras.backend.round(keras.backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
    "    # Calculate false negatives\n",
    "    fn = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
    "    # Calculate precision\n",
    "    p = tp / (tp + fp + keras.backend.epsilon())\n",
    "    # Calculate recall\n",
    "    r = tp / (tp + fn + keras.backend.epsilon())\n",
    "    # Calculate fbeta, averaged across each class\n",
    "    bb = beta ** 2\n",
    "    # F-beta score final calculation\n",
    "    fbeta_score = keras.backend.mean((1 + bb) * (p * r) / (bb * p + r + keras.backend.epsilon()))\n",
    "    # Return statement\n",
    "    return(fbeta_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All ones prediction\n",
    "\n",
    "A standard benchmark to decide whether any machine learning model is useful is if our algorithm can get a better score than if we just blindly predict 1 for each and every image we get. The function below implements this all-1s algorithm and returns the score. If the deep learning model that we create later on can do better than this than it is adding value over and above this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(trainX, trainY, testX, testY):\n",
    "    '''\n",
    "    Input: Training and test datasets and labels\n",
    "    Output: Training and test score if we just always predict ones\n",
    "    '''\n",
    "    # Make all one predictions\n",
    "    train_yhat = np.asarray([np.ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\n",
    "    test_yhat = np.asarray([np.ones(testY.shape[1]) for _ in range(testY.shape[0])])\n",
    "    \n",
    "    # Evaluate predictions with sklearn\n",
    "    train_score = fbeta_score(trainY, train_yhat, 2, average='samples')\n",
    "    test_score = fbeta_score(testY, test_yhat, 2, average='samples')\n",
    "    print('All Ones (sklearn): train=%.3f, test=%.3f' % (train_score, test_score))\n",
    "\n",
    "    # Evaluate predictions with keras\n",
    "    train_score = fbeta(keras.backend.variable(trainY), keras.backend.variable(train_yhat))\n",
    "    test_score = fbeta(keras.backend.variable(testY), keras.backend.variable(test_yhat))\n",
    "    print('All Ones (keras): train=%.3f, test=%.3f' % (train_score, test_score))\n",
    "    \n",
    "    # Return the train and test sets for future use\n",
    "    return(train_score, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline convolutional neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model for the Planet dataset\n",
    "import sys\n",
    "from numpy import load\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model\n",
    "\n",
    "The model below is a convolutional neural network with maximum pooling and same padding. It uses **sigmoid activation** in the output layer and **RELU activation** in the hidden layers. The output of the function is a **keras model object.** We use the **binary cross entropy loss**. We also track the **F-beta score** metric according to the custom function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(in_shape=(32, 32, 3), out_shape=17, lr = 0.05, momentum = 0.9):\n",
    "    '''\n",
    "    Input: Input shape and output shape\n",
    "    Output: Keras model object\n",
    "    '''\n",
    "    # Define model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(out_shape, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    opt = SGD(lr=lr, momentum=momentum)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta, keras.metrics.accuracy])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create diagnostic plots\n",
    "\n",
    "The function below creates diagnostic plots for easier checking at the end of training. It plots both the loss function and the F-beta score function for training and test. Example plots can be seen in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pacakges\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_diagnostics(history):\n",
    "    '''\n",
    "    Input: Keras history project\n",
    "    Output: Display diagnostic learning curves\n",
    "    '''\n",
    "    # Plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
    "    \n",
    "    # Plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Fbeta')\n",
    "    pyplot.plot(history.history['fbeta'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_fbeta'], color='orange', label='test')\n",
    "    \n",
    "    # Save plot to file\n",
    "    filename = sys.argv[0].split('/')[-1]\n",
    "    pyplot.savefig(filename + '_plot.png')\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensorboard_directory():\n",
    "    '''\n",
    "    Input: N/A\n",
    "    Output: Tensorboard directory path\n",
    "    '''\n",
    "    root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    \n",
    "    return(os.path.join(root_logdir, run_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the network \n",
    "\n",
    "The code below runs the training algorithm for the neural network. It calls the **load_dataset()** function to create training and testing data-sets. It then specifies to **checkpoint** variables. The first checkpoint variable tells **keras** to save a copy of the latest results and estimates to a file called **my_keras_model.h5**. The next checkpoint **early_stopping_cb** specifies that we want training to stop if there is no improvement seen for a particular no. of batches. The **patience** parameter is set to 10. This means that training will automatically stop if there is no improvement seen for 10 successive batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_harness(epochs = 2, verbose = 1, batch_size = 128, lr = 0.05, momentum = 0.9, \n",
    "                     in_shape=(32, 32, 3), out_shape=17):\n",
    "    '''\n",
    "    Input: None\n",
    "    Output: None\n",
    "    Run the test harness for evaluating a model\n",
    "    '''\n",
    "    # Load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    \n",
    "    # Run benchmark\n",
    "    results = benchmark(trainX, trainY, testX, testY)\n",
    "    \n",
    "    # Create data generator\n",
    "    train_datagen = ImageDataGenerator(rescale=1.0/255.0, horizontal_flip=True, \n",
    "                                       vertical_flip=True, rotation_range=90)\n",
    "    test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "    \n",
    "    # Add checkpoints for regular saving\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Add TensorBoard logging\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(make_tensorboard_directory())\n",
    "\n",
    "    # Prepare iterators\n",
    "    train_it = train_datagen.flow(trainX, trainY, batch_size=batch_size)\n",
    "    test_it = test_datagen.flow(testX, testY, batch_size=batch_size)\n",
    "    \n",
    "    # Define model\n",
    "    model = define_model(in_shape, out_shape, lr, momentum)\n",
    "    \n",
    "    # Fit model\n",
    "    history = model.fit_generator(train_it, \n",
    "                                  steps_per_epoch=len(train_it), \n",
    "                                  validation_data=test_it, validation_steps=len(test_it), \n",
    "                                  callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb], \n",
    "                                  epochs=epochs, \n",
    "                                  verbose=1)\n",
    "    # Evaluate model\n",
    "    loss, fbeta, accuracy = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n",
    "    print('> loss=%.3f, fbeta=%.3f, accuracy=%.3f' % (loss, fbeta, accuracy))\n",
    "    \n",
    "    # Learning curves\n",
    "    summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28335, 32, 32, 3) (28335, 17) (12144, 32, 32, 3) (12144, 17)\n",
      "All Ones (sklearn): train=0.484, test=0.484\n",
      "All Ones (keras): train=0.484, test=0.484\n",
      "Epoch 1/2\n",
      "222/222 [==============================] - 519s 2s/step - loss: 0.2371 - fbeta: 0.6478 - accuracy: 1.0380e-05 - val_loss: 0.1961 - val_fbeta: 0.6998 - val_accuracy: 9.6877e-06\n",
      "Epoch 2/2\n",
      "222/222 [==============================] - 529s 2s/step - loss: 0.2048 - fbeta: 0.7078 - accuracy: 8.0964e-05 - val_loss: 0.1691 - val_fbeta: 0.7391 - val_accuracy: 1.5016e-04\n",
      "95/95 [==============================] - 54s 565ms/step\n",
      "> loss=0.186, fbeta=0.739, accuracy=0.000\n"
     ]
    }
   ],
   "source": [
    "# Entry point, run the test harness\n",
    "run_test_harness()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
